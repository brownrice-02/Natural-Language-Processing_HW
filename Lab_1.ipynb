{"cells":[{"cell_type":"markdown","metadata":{"id":"T4-J5jkcatiQ"},"source":["**Lab1 Reminder: **\n","1. Use small data to develop your code.\n","2. Relink to the demomstration code can check the output."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VISir4btCqU9","outputId":"d1ae424b-8480-42b4-b4f4-a45f4f338948","executionInfo":{"status":"ok","timestamp":1676014461227,"user_tz":-480,"elapsed":7113,"user":{"displayName":"李芷萱","userId":"11306129375623219406"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting rouge\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from rouge) (1.15.0)\n","Installing collected packages: rouge\n","Successfully installed rouge-1.0.1\n"]}],"source":["!pip install rouge"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QqVxwKpgCe0a","outputId":"24689ff8-3785-466f-e2bc-8b6ede2a7a4e","executionInfo":{"status":"ok","timestamp":1676014468334,"user_tz":-480,"elapsed":3739,"user":{"displayName":"李芷萱","userId":"11306129375623219406"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["# For debugging\n","import pdb\n","\n","# For checking progress\n","from tqdm import tqdm\n","\n","# For loading data\n","import pandas as pd\n","\n","# For tokenizaton\n","import nltk\n","from nltk import word_tokenize, sent_tokenize\n","from nltk import ngrams\n","nltk.download('punkt')\n","\n","# For building n-gram model\n","from collections import Counter, namedtuple\n","import numpy as np\n","\n","# For evaluation \n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from rouge import Rouge \n","\n","# For pos tagging\n","nltk.download('averaged_perceptron_tagger')"]},{"cell_type":"markdown","metadata":{"id":"r4r6vYRwwV6m"},"source":["# Part 1. Data Preprocessing\n","1. show the top-10 common words and their counts before/after preprocessing\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WXl7m78Oxz9d"},"source":["## Functions and Classes\n","*  Remove the punctuations\n","*  Lower the cases\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fy2bDYorrKxW"},"outputs":[],"source":["def get_corpus():\n","  \"\"\" Reads and formats the corpus.\n","\n","  Returns:\n","    corpus (list[str]):\n","      A list of sentences in the corpus.\n","  \"\"\"\n","  df = pd.read_csv('https://raw.githubusercontent.com/yilihsu/NLP110/main/data_tiny.csv')\n","  corpus = df.content.to_list()\n","  return corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqW8SUq_xzCm"},"outputs":[],"source":["def preprocess(documents):\n","  \"\"\" Preprocesses the corpus.\n","  \n","  Args:\n","    documents (list[str]):\n","      A list of sentences in the corpus.\n","  Returns:\n","    cleaned_documents (list[str]):\n","      A list of cleaned sentences in the corpus.3\n","  \"\"\"\n","  cleaned_documents = []\n","  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~”'''\n","  for doc in documents:\n","    # Tokenizes the sentence\n","    sents = sent_tokenize(doc)\n","\n","    for sent in sents:\n","      #pdb.set_trace() # delete this line for the final version\n","\n","      # Removes the punctuations [TODO 1]\n","      for i in range(len(punc)):\n","        sent = sent.replace(punc[i], '')\n","      #sent = sent.translate(str.maketrans('', '', punc))\n","      # Lowers the case\n","      sent = sent.lower() \n","      \n","      cleaned_documents.append(sent)\n","\n","  #print(cleaned_documents[:5])\n","  return cleaned_documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLxv8uXpxvC2"},"outputs":[],"source":["# Compute word frequency\n","def get_vocab(documents):\n","  \"\"\" Gets the vocabulary from the corpus.\n","  \n","  Args:\n","    documents (list[str]):\n","      A list of sentences in the corpus\n","  Returns:\n","    vocabulary (collections.Counter)\n","  \"\"\"\n","  vocabulary = Counter()\n","\n","  for doc in tqdm(documents):\n","    tokens = word_tokenize(doc)\n","    vocabulary.update(tokens)\n","\n","  return vocabulary"]},{"cell_type":"markdown","metadata":{"id":"0Au2w1vJjHqP"},"source":["## Executions"]},{"cell_type":"code","source":[],"metadata":{"id":"yIEvVTVEAaZg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IkCSSWp3j65G"},"source":["### 1. Show the top-10 common words and their counts before/after preprocessing\n"]},{"cell_type":"code","source":[],"metadata":{"id":"z18A2wGfAaFb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sEDgtK3RxFXt","outputId":"d5145200-b5fc-482a-c55f-45841069d059","executionInfo":{"status":"ok","timestamp":1675834902145,"user_tz":-480,"elapsed":11124,"user":{"displayName":"李芷萱","userId":"11306129375623219406"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 20000/20000 [00:04<00:00, 4684.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n"," Before preprocessing: [('.', 16981), ('the', 9885), (',', 7788), ('to', 7005), ('!', 6642), ('a', 5596), ('is', 5111), ('?', 4640), ('and', 4584), ('you', 4463)]\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 34977/34977 [00:04<00:00, 7051.25it/s]"]},{"output_type":"stream","name":"stdout","text":["\n"," After preprocesing: [('the', 11175), ('to', 7117), ('a', 5847), ('you', 5325), ('is', 5245), ('and', 5087), ('of', 4492), ('i', 3231), ('in', 3203), ('it', 3190)]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["# Read data\n","raw_documents = get_corpus()\n","\n","# Build vocabulary\n","vocab = get_vocab(raw_documents).most_common(10)\n","print('\\n Before preprocessing:', vocab)\n","\n","# Build vocabulary after preprocessing\n","documents = preprocess(raw_documents)\n","vocab = get_vocab(documents).most_common(10)\n","print('\\n After preprocesing:', vocab)"]},{"cell_type":"markdown","metadata":{"id":"N-mCeiF3inh-"},"source":["# Part 2. N-Gram Model and POS Tagging\n","1. Build 2-gram / 4-gram model by processed dataset\n","2. Show the top-5 probable next words and their probability after initial token ‘\\<s\\>’ by 2-gram model\n","3. Generate a sentence with 2-gram model and find the POS taggings\n","4. Generate a sentence with 4-gram model and find the POS taggings\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Q7MRk6EMjlKL"},"source":["## Functions and Classes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsStQG516jC6"},"outputs":[],"source":["class Ngram_model(object):\n","  \"\"\" Ngram model implementation.\n","\n","  Attributes:\n","    n (int):\n","      The number of grams to be considered.\n","    model (dict):\n","      The ngram model.\n","  \"\"\"\n","  def __init__(self, documents, N=2):\n","    self.n = N\n","    self.model = self.get_ngram_model(documents)\n","\n","  def get_ngram_model(self, documents):\n","    N = self.n\n","    ngram_model = dict()\n","    full_grams = list()\n","    grams = list()\n","    Word = namedtuple('Word', ['word', 'prob'])\n","\n","    # for each sentence in documents [TODO 2]\n","    for doc in documents:\n","      \n","      # Tokenizes to words [TODO 3]\n","      sents = word_tokenize(doc)\n","      \n","      # Append (N-1) start tokens '<s>' and an end token '<\\s>' [TODO 4]\n","      sents = sents + ['<\\s>']\n","      for i in range(N-1):\n","        sents = ['<s>'] + sents\n","\n","      # Calculates numerator (construct list with full grams, i.e., N-grams) [TODO 5]\n","      #full_grams =  nltk.bigrams(sents)\n","      full_grams += ngrams(sents, N)\n","\n","      # Calculate denominator (construct list with grams, i.e., (N-1)-grams) [TODO 6]\n","      grams += ngrams(sents, N-1)\n","      \n","    # Count the occurence frequency of each gram\n","    # Take 2-gram model as example:\n","    #   full_grams -> list[('a', 'gram'),('other', 'gram'), ...]\n","    #   grams -> list[('a'), ('other'), ('gram'), ...]\n","    #   full_gram_counter -> dict{('a', 'gram'):frequency_1, ('other','gram'):frequency_2, ...}\n","    #   gram_counter -> dict{('a'):frequency_1, ('gram'):frequency_2, ...}\n","    full_gram_counter = Counter(full_grams)\n","    gram_counter = Counter(grams)\n","\n","    # Build model\n","    # Take 2-gram model as example:\n","    #   { '<s>': [tuple(word='i', prob=0.6), tuple(word='the', prob=0.2), ...],\n","    #   'i': [tuple(word='am', prob=0.7), tuple(word='want', prob=0.1), ...],\n","    #    ... }\n","    for key in full_gram_counter:\n","      word = ''.join(key[:N-1])\n","\n","      if word not in ngram_model:\n","        ngram_model.update({word: set()})\n","\n","      # next_word_prob -> float\n","      next_word_prob = full_gram_counter[key] / gram_counter[key[:N-1]]\n","      w = Word(key[-1], next_word_prob)\n","      ngram_model[word].add(w)\n","\n","    # Sort the result by frequency\n","    for word, ng in ngram_model.items():\n","      ngram_model[word] = sorted(ng, key=lambda x: x.prob, reverse=True)\n","\n","    return ngram_model\n","\n","\n","  def predict_sent(self, text=None, max_len=30):\n","    \"\"\" Predicts a sentence with the ngram model.\n","\n","    Args:\n","      text (string or list[string])\n","    Returns:\n","      A prediction string.\n","    \"\"\"\n","\n","    N = self.n\n","    backup_tokens = ['<s>']*(N-1)\n","    if not text:\n","      tokens = backup_tokens\n","      output = []\n","\n","    elif type(text)==str:\n","      tokens = backup_tokens + text.split(' ')\n","      tokens = tokens[-(N-1):]\n","      if not self.check_existence(tokens):\n","        return \n","      output = tokens\n","\n","    elif type(text) == list:\n","      tokens = backup_tokens + text\n","      tokens = tokens[-(N-1):]\n","      if not self.check_existence(tokens):\n","        return\n","      output = tokens\n","\n","    else:\n","      print('[Error] the input text must be string or list of string')\n","      return\n","\n","    for i in range(max_len):\n","      possible_words = list(self.model[''.join(tokens)])\n","      probs = [word.prob for word in possible_words]\n","      words = [word.word for word in possible_words]\n","      next_word = np.random.choice(words, 1, p=probs)[0]\n","      tokens = tokens[1:] + [next_word]\n","\n","      if next_word == '<\\\\s>':\n","        break\n","\n","      output.append(next_word)\n","    return ' '.join(output)\n","\n","  def predict_next(self, text=None, top=5):\n","    \"\"\" Predicts next word with the ngram model.\n","\n","    Args:\n","      text (string or list[string])\n","\n","    Returns:\n","      possible_next_words (list[namedtuple]):\n","        A list of top few possible next words.\n","    \"\"\"\n","\n","    N = self.n\n","    backup_tokens = ['<s>']*(N-1)\n","    if not text:\n","      tokens = backup_tokens\n","\n","    elif type(text)==str:\n","      tokens = backup_tokens + text.split(' ')\n","      tokens = tokens[-(N-1):]\n","      if not self.check_existence(tokens):\n","        return \n","\n","    elif type(text) == list:\n","      tokens = backup_tokens + text\n","      tokens = tokens[-(N-1):]\n","      if not self.check_existence(tokens):\n","        return\n","    else:\n","      print('[Error] the input text must be string or list of string')\n","\n","    possible_next_words = self.model[''.join(tokens)][:top]\n","    possible_next_words = [(word.word, word.prob) for word in possible_next_words]\n","\n","    return possible_next_words\n","\n","  def check_existence(self, tokens):\n","    if not ''.join(tokens) in self.model.keys():\n","      print('[Error] the input text {} not in the vocabulary'.format(tokens))\n","      return False\n","    else:\n","      return True"]},{"cell_type":"markdown","metadata":{"id":"5_CgC8RKjhd0"},"source":["## Executions"]},{"cell_type":"markdown","metadata":{"id":"WcJhv21YCE8I"},"source":["### 1. Build 2-gram/4-gram model by processed dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h17uKROx-FZG"},"outputs":[],"source":["twogram = Ngram_model(documents, N=2)"]},{"cell_type":"markdown","metadata":{"id":"QOV203H4CNg7"},"source":["### 2. Show the top-5 probable next words and their probability after initial token \\'\\<s\\>\\'  by 2-gram model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CoD7wcAJ-PYm","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f65c3f34-0d3b-463c-dec4-0c5a7224e967","executionInfo":{"status":"ok","timestamp":1675834934164,"user_tz":-480,"elapsed":301,"user":{"displayName":"李芷萱","userId":"11306129375623219406"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Next word predictions of two gram model: [('i', 0.05280612974240215), ('the', 0.03102038482431312), ('you', 0.030248448980758784), ('<\\\\s>', 0.029190610972925066), ('they', 0.019784429768133344)]\n"]}],"source":["output = twogram.predict_next(text='<s>', top=5)\n","print('Next word predictions of two gram model:', output)"]},{"cell_type":"markdown","metadata":{"id":"C51gJXJO7JBi"},"source":["### 3. Generate a sentence with 2-gram model and find the POS taggings\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBvwvE6q7Vpl","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5e74544e-45dc-4965-adf8-efcda8ac9721","executionInfo":{"status":"ok","timestamp":1675834936787,"user_tz":-480,"elapsed":281,"user":{"displayName":"李芷萱","userId":"11306129375623219406"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Generation results of two gram model: might he sees the rich\n"]},{"output_type":"execute_result","data":{"text/plain":["[('might', 'MD'),\n"," ('he', 'PRP'),\n"," ('sees', 'VBZ'),\n"," ('the', 'DT'),\n"," ('rich', 'JJ')]"]},"metadata":{},"execution_count":10}],"source":["output = twogram.predict_sent(max_len=30)\n","print('Generation results of two gram model:', output)\n","nltk.pos_tag(word_tokenize(output))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W-n6loDYC8qn"},"outputs":[],"source":["def evaluation(generated_sentence, reference_sentence):\n","\n","  if len(generated_sentence) <= 1: \n","    raise RuntimeError('Not enough length to evaluate, please try again with another generation.')\n","\n","  rouge = Rouge()\n","  smoothie = SmoothingFunction().method4\n","  \n","  # Hint: please refer to the import function in the beginning of this notebook\n","  # Given the smoothing_function=smoothie, weights=(1,0,0,0), please calculate BLEU-1 score with function call [TODO 7]\n","  bleu_score = sentence_bleu(reference_sentence, generated_sentence, smoothing_function=smoothie, weights=(1,0,0,0))\n","  # Calculates ROUGE-1 f score with function call [TODO 8]\n","  generated_sentence = \" \".join(generated_sentence)\n","  reference_sentence = \" \".join(reference_sentence)\n","  rouge_score = rouge.get_scores(generated_sentence, reference_sentence)\n","  rouge_score = rouge_score[0][\"rouge-1\"][\"f\"]\n","  \n","  return bleu_score, rouge_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iYA9Yh8qEhEc","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb61efa7-034b-480e-983d-254855eff284","executionInfo":{"status":"ok","timestamp":1675834942894,"user_tz":-480,"elapsed":5,"user":{"displayName":"李芷萱","userId":"11306129375623219406"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Generated results given the text:  ['is', 'coming', 'from', 'that', 'in', 'america', 'and', 'fakenewscnn']\n","Reference sentence:  ['is', 'a', 'party', 'day']\n","bleu score:  0\n","rouge score:  0.16666666222222234\n"]}],"source":["given_text = \"today is\"\n","references = ['is', 'a', 'party', 'day']\n","output = twogram.predict_sent(text=given_text, max_len=30)\n","output = word_tokenize(output)\n","print(\"Generated results given the text: \", output)\n","print(\"Reference sentence: \", references)\n","\n","bleu_score, rouge_score = evaluation(output, references)\n","print(\"bleu score: \", bleu_score)\n","print(\"rouge score: \", rouge_score)"]},{"cell_type":"code","source":[],"metadata":{"id":"pWCalWsiHWBt"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}